nav:
  - Glossary: index.md
  - 线性模型: linear-models.ipynb
  - 决策树: decision-tree.ipynb
  - 神经网络:
    - 激活函数: activation.md
    - 损失函数: loss.md
    - 归一化: normalization.md
    - MLP: mlp.md
    - transformer:
      - 注意力机制: attention.md
      - 编码器与解码器: encoder-decoder.md
      - transformer: transformer.ipynb
      - 位置编码: positional-embedding.md
      - transformer变种: transformer-variants.md
  - 概率分布采样:
    - Box-Muller变换: box-muller-transform.ipynb
    - 拒绝采样: rejection-sampling.ipynb
  - 贝叶斯优化:
    - 高斯过程: gaussian-process.ipynb
    - 算法: bayesian-optimization.ipynb
  - 自编码器:
    - Autoencoder: autoencoder.ipynb
    - Variational AE: vae.ipynb
  - 生成对抗网络:
    - GAN: gan.ipynb
    - Wasserstein GAN: wgan.ipynb
    - Conditional GAN: cgan.ipynb
  - 语言模型:
    - BERT: bert.ipynb
    - Llama-2: llama-2.ipynb
    - KV-Cache: llm-inference.md
  - 强化学习:
    - 目录: reinforcement-learning.md
    - 强化学习基础概念: rl-1-basics.md
    - "RL Lab 1: Environment": rl_env.py
    - 动态规划: rl-2-dp-methods.md
    - "RL Lab 2: Value Iteration": rl-value-iteration.ipynb
    - "RL Lab 3: Policy Iteration": rl-policy-iteration.ipynb
    - 时序差分学习: rl-3-temporal-difference-learning.md
    - "RL Lab 4: Temporal Difference Learning": rl-td-learning.ipynb
    - 深度Q网络: rl-4-dqn.md
    - "RL Lab 5: Jimbo Game": rl_jimbo_env.py
    - "RL Lab 6: Deep Q Networks": rl-q-networks.ipynb
    - "RL Lab 7: Dueling DQN": rl-dueling-q-networks.ipynb
    - 策略梯度方法: rl-5-policy-gradient-methods.md
    - "RL Lab 8: REINFORCE": rl-reinforce.ipynb
    - "RL Lab 9: Advantage Actor-Critic": rl-a2c.ipynb
    - 多臂老虎机: multi-armed-bandit.ipynb
    - 置信上界算法: upper-confidence-bound.md
