{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "\n",
    "以下使用Python实现基于线性关系的各种机器学习模型\n",
    "\n",
    "## 多元线性回归\n",
    "\n",
    "多元线性回归的一般形式如下：\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol y} = f(\\boldsymbol x) = \\boldsymbol w^\\top \\boldsymbol x + b\n",
    "$$\n",
    "\n",
    "当样本矩阵$\\boldsymbol X$满秩时，最优解为\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bmX}{\\boldsymbol X}\n",
    "\\hat{\\boldsymbol w}^* = (\\bmX^\\top\\bmX)^{-1}\\bmX^\\top \\boldsymbol y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.01568584,  1.13091673,  0.73591806, -0.38206408, -2.28335669]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "X = np.random.normal(5, 3, size=(10, 4))\n",
    "X = np.concatenate((X, np.ones((10, 1))), axis=1) # b as bias\n",
    "y = np.random.normal(5, 3, size=(10, 1))\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "linear_regression(X, y).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数为均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.457914535338647"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def mse_loss(X, y, w):\n",
    "    return np.mean((X @ w - y) ** 2)\n",
    "\n",
    "mse_loss(X, y, linear_regression(X, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本矩阵非满秩时，存在多个满足训练集的模型，此时可以在优化目标中加入正则化项，如L2-norm即加入权重的平方之和。然后使用梯度下降等数值方式进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.08334051  0.9085506   0.62607314 -0.42271554 -0.21530994]] 5.572712322367768\n"
     ]
    }
   ],
   "source": [
    "class SGD():\n",
    "    def __init__(self, d, lr=0.001, epochs=100):\n",
    "        self.d = d\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __call__(self, X, y):\n",
    "        w = np.random.normal(0, 1, size=(X.shape[1], 1))\n",
    "        for _ in range(self.epochs):\n",
    "            w -= self.lr * self.d(X, y, w)\n",
    "        return w\n",
    "\n",
    "optim_l2 = SGD(lambda X, y, w: X.T @ (X @ w - y) + 0.1 * w)\n",
    "\n",
    "w = optim_l2(X, y)\n",
    "print(w.T, mse_loss(X, y, w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对率回归可以将线性模型应用到二分类问题上。对率回归需要用到logit函数将连续的回归值映射到 $(0, 1)$ 上\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.random.normal(5, 3, size=(100, 4))\n",
    "X = np.concatenate((X, np.ones((100, 1))), axis=1) # b as bias\n",
    "y = np.random.randint(0, 2, size=(100, 1))\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def precision(X, y, w):\n",
    "    return np.mean((sigmoid(X @ w) > 0.5) == y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用极大似然法可以得到对率回归的优化目标函数\n",
    "\n",
    "$$\n",
    "l(\\boldsymbol w) = \\sum_{i=1}^m \\left(-\\boldsymbol y_i\\boldsymbol \\beta^\\top \\boldsymbol x_i + \\ln \\left(1 + e^{\\boldsymbol \\beta^\\top \\boldsymbol x_i}\\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.19957716  0.17344992  0.13465464  0.16220733 -1.38798006]] 0.57\n"
     ]
    }
   ],
   "source": [
    "optim_logit = SGD(lambda X, y, w: X.T @ (sigmoid(X @ w) - y))\n",
    "\n",
    "w = optim_logit(X, y)\n",
    "print(w.T, precision(X, y, w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "LDA是线性判别分析的简称，属于分类算法。该算法的核心思路为将样本点投影到 $n$ 维空间的平面上，通过选择平面，最小化同一类别内样本点投影的距离，同时最大化不同类别样本点投影的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.49"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.random.normal(5, 3, size=(100, 5))\n",
    "y = np.random.randint(0, 2, size=(100, 1))\n",
    "def cov(X, a, b):\n",
    "    return np.mean((X[:, a] - np.mean(X[:, a])) * (X[:, b] - np.mean(X[:, b])))\n",
    "\n",
    "def cov_matrix(X):\n",
    "    return np.array([\n",
    "        [\n",
    "            cov(X, i, j)\n",
    "            for i in range(X.shape[1])\n",
    "        ]\n",
    "        for j in range(X.shape[1])\n",
    "    ])\n",
    "\n",
    "sw = cov_matrix(X[y[:, 0] == 0]) + cov_matrix(X[y[:, 0] == 1])\n",
    "mu0, mu1 = np.mean(X[y[:, 0] == 0], axis=0), np.mean(X[y[:, 0] == 1], axis=0)\n",
    "w = np.linalg.inv(sw) @ (mu0 - mu1).reshape(-1, 1)\n",
    "c0, c1 = w.T @ mu0, w.T @ mu1\n",
    "\n",
    "def precision(X, y, w):\n",
    "    return np.mean((X @ w > (c0 + c1) / 2) == y)\n",
    "\n",
    "precision(X, y, w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
