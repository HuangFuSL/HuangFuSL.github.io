{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 多臂老虎机\n",
    "\n",
    "多臂老虎机（Multi-armed bandit）问题来自于现实中的老虎机问题。每台老虎机都有一个拉杆，拉动拉杆后，老虎机会随机给出一个奖励。玩家的任务是在有限的尝试次数内尽可能获得更多的奖励。\n",
    "\n",
    "多臂老虎机问题的数学定义为：已知一组$K\\in \\mathbb N^{+}$个定义在$\\mathbb R$上的概率分布$\\mathcal B = \\{D_1, \\ldots, D_K\\}$，其中$D_k$表示第$k$个老虎机给出奖励的概率分布，$D_k$的均值为$\\mu_k$。在第$t$轮中，智能体从$\\mathcal B$中随机选出一个分布$D^{(t)}\\in\\mathcal B$，并从中采样一个奖励$X^{(t)}\\sim D^{(t)}$。智能体的目标是在有限的轮次内最大化累积奖励$\\sum_{t=1}^T X^{(t)}$。后悔值$\\rho$定义为最优策略的$T$轮累计奖励的期望值与智能体的$T$轮累计奖励之差，即\n",
    "\n",
    "$$\n",
    "\\rho = T\\max\\{\\mu_k\\} - \\mathbb E\\left[\\sum_{t=1}^T X^{(t)}\\right]\n",
    "$$\n",
    "\n",
    "用Python可以简单实现一个与智能体交互的多臂老虎机环境："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-1.435877863923347\n"
     ]
    }
   ],
   "source": [
    "import itertools\n",
    "import random\n",
    "from typing import List, Optional\n",
    "\n",
    "from scipy.stats import norm\n",
    "\n",
    "\n",
    "class MultiArmedBandit():\n",
    "    def __init__(self, means, stds: Optional[List[float]] = None):\n",
    "        self.means = means\n",
    "        if stds is not None:\n",
    "            if len(stds) != len(means):\n",
    "                raise ValueError(\n",
    "                    'The length of means and stds should be the same.')\n",
    "            self.stds = stds\n",
    "        else:\n",
    "            self.stds = [1.0] * len(means)\n",
    "        self.k = len(means)\n",
    "        self.dists = [\n",
    "            norm(loc=mean, scale=std) for mean, std in zip(means, self.stds)\n",
    "        ]\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.k\n",
    "\n",
    "    @property\n",
    "    def optimal_reward(self) -> float:\n",
    "        return max(self.means)\n",
    "\n",
    "    def pull(self, action: int) -> float:\n",
    "        return self.dists[action].rvs()\n",
    "\n",
    "rewards = [*range(0, 10, 2)]\n",
    "stds = [*range(1, 6)]\n",
    "args = itertools.product(rewards, stds)\n",
    "bandit = MultiArmedBandit(*zip(*args))\n",
    "print(bandit.pull(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 算法\n",
    "\n",
    "多臂老虎机算法的核心是在探索（Exploration）和利用（Exploitation）之间寻找平衡。探索可能会找到（相较于当前已知的）更好的奖励，而利用则是直接选择当前已知的最好的奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import abc\n",
    "from typing import Tuple\n",
    "\n",
    "class AbstractAgent(abc.ABC):\n",
    "    def __init__(self, T: int, k: int):\n",
    "        self.T = T\n",
    "        self.k = k\n",
    "\n",
    "        self.history: List[Tuple[int, float]] = []\n",
    "\n",
    "    @abc.abstractmethod\n",
    "    def strategy(self) -> int:\n",
    "        pass\n",
    "\n",
    "    def run(self, bandit: MultiArmedBandit) -> float:\n",
    "        rewards = []\n",
    "        for t in range(self.T):\n",
    "            action = self.strategy()\n",
    "            if action >= self.k:\n",
    "                raise ValueError('Invalid action.')\n",
    "\n",
    "            reward = bandit.pull(action)\n",
    "            self.history.append((action, reward))\n",
    "            rewards.append(reward)\n",
    "        return self.T * bandit.optimal_reward - sum(rewards)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 完全随机策略\n",
    "\n",
    "在完全随机策略下，智能体随机选择一个臂，获取奖励，后悔值的期望为$\\mathbb E\\rho = \\sum_{k=1}^K (\\mu^* - \\mu_k)$。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def show_result(x: List[float], name: str) -> str:\n",
    "    x_array = np.array(x)\n",
    "    return f'{name} - Mean: {x_array.mean():.2f} Std: {x_array.std():.2f}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Random Agent - Mean: 399.66 Std: 44.19'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class RandomAgent(AbstractAgent):\n",
    "    def strategy(self) -> int:\n",
    "        return random.randint(0, self.k - 1)\n",
    "\n",
    "random_agents = [RandomAgent(T=100, k=len(bandit)) for _ in range(1000)]\n",
    "regrets = [agent.run(bandit) for agent in random_agents]\n",
    "show_result(regrets, 'Random Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### $\\epsilon$-贪心算法\n",
    "\n",
    "$\\epsilon$-贪心算法是最简单的多臂老虎机算法之一。在每一轮中，以$\\epsilon$的概率随机选择一个老虎机，以$1-\\epsilon$的概率选择当前已知的最好的老虎机。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Epsilon-Greedy Agent - Mean: 155.49 Std: 79.32'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from typing import Dict\n",
    "\n",
    "class EpsilonGreedyAgent(AbstractAgent):\n",
    "    def __init__(self, T: int, k: int, epsilon: float):\n",
    "        super().__init__(T, k)\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def _random(self) -> int:\n",
    "        return random.randint(0, self.k - 1)\n",
    "\n",
    "    def _greedy(self) -> int:\n",
    "        history_dict: Dict[int, List[float]] = {}\n",
    "        for action, reward in self.history:\n",
    "            if action not in history_dict:\n",
    "                history_dict[action] = []\n",
    "            history_dict[action].append(reward)\n",
    "        return max(\n",
    "            history_dict,\n",
    "            key=lambda x: sum(history_dict[x]) / len(history_dict[x])\n",
    "        )\n",
    "\n",
    "    def strategy(self) -> int:\n",
    "        if random.random() < self.epsilon or not self.history:\n",
    "            return self._random()\n",
    "        return self._greedy()\n",
    "\n",
    "\n",
    "epsilon_greedy = [\n",
    "    EpsilonGreedyAgent(T=100, k=len(bandit), epsilon=0.2) for _ in range(1000)\n",
    "]\n",
    "regrets = [agent.run(bandit) for agent in epsilon_greedy]\n",
    "show_result(regrets, 'Epsilon-Greedy Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 置信上界算法\n",
    "\n",
    "置信上界算法（Upper Confidence Bound, UCB）是一种基于置信区间的多臂老虎机算法。在每一轮中，智能体选择一个老虎机，使得该老虎机的置信区间上界最大。置信区间上界的计算公式为\n",
    "\n",
    "$$\n",
    "\\bar X_k + c\\cdot \\sqrt{\\frac{\\log t}{N_k}}\n",
    "$$\n",
    "\n",
    "其中$\\bar X_k$为第$k$个老虎机的平均奖励，$N_k$为第$k$个老虎机被选择的次数，$c$为一个常数，$t$为当前轮次。其思想在于，智能体会首先遍历所有臂，之后优先选择那些被选择次数较少的老虎机，以便更好地估计其奖励。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UCB Agent - Mean: 116.76 Std: 29.87'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "class UCBAgent(AbstractAgent):\n",
    "    def __init__(self, T: int, k: int, c: float):\n",
    "        super().__init__(T, k)\n",
    "        self.c = c\n",
    "\n",
    "    def ucb(self, action: int) -> float:\n",
    "        n = len([a for a, _ in self.history if a == action])\n",
    "        if n == 0:\n",
    "            return float('inf')\n",
    "        mean = sum(r for a, r in self.history if a == action) / n\n",
    "        std = math.sqrt(math.log(len(self.history)) / n)\n",
    "        return mean + self.c * std\n",
    "\n",
    "    def strategy(self) -> int:\n",
    "        # If there are still unselected actions, select one of them.\n",
    "        if len(self.history) < self.k:\n",
    "            return len(self.history)\n",
    "\n",
    "        return max(range(self.k), key=self.ucb)\n",
    "\n",
    "ucb_agents = [UCBAgent(T=100, k=len(bandit), c=0.3) for _ in range(1000)]\n",
    "regrets = [agent.run(bandit) for agent in ucb_agents]\n",
    "show_result(regrets, 'UCB Agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thompson采样\n",
    "\n",
    "Thompson采样是一种基于贝叶斯推断的多臂老虎机算法，需要已知每个老虎机的奖励服从某种分布。在每一轮中，智能体从每个老虎机的奖励分布中采样一个奖励，选择奖励最大的老虎机。根据环境的反馈，更新每个老虎机的后验奖励分布。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Thompson Sampling Agent - Mean: 231.88 Std: 49.94'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.stats import invgamma\n",
    "\n",
    "class ThompsonSamplingAgent(AbstractAgent):\n",
    "    def __init__(self, T: int, k: int):\n",
    "        super().__init__(T, k)\n",
    "\n",
    "        # We know that the reward is gaussian, so we can use normal-inverse-gamma\n",
    "        self.mu = [0.0] * k\n",
    "        self.lambd = [1.0] * k\n",
    "        self.alpha = [1.0] * k\n",
    "        self.beta = [1.0] * k\n",
    "\n",
    "    def update(self, action: int, reward: float) -> None:\n",
    "        self.mu[action] = (\n",
    "            self.lambd[action] * self.mu[action] + reward\n",
    "        ) / (self.lambd[action] + 1)\n",
    "        self.lambd[action] += 1\n",
    "        self.alpha[action] += 0.5\n",
    "        self.beta[action] += (\n",
    "            self.lambd[action] * (reward - self.mu[action]) ** 2\n",
    "        ) / (self.lambd[action] + 1) / 2\n",
    "\n",
    "    def strategy(self) -> int:\n",
    "        if self.history:\n",
    "            self.update(*self.history[-1])\n",
    "\n",
    "        samples = [\n",
    "            norm.rvs(\n",
    "                loc=mu, scale=invgamma.rvs(a, scale=b) / math.sqrt(lambd)\n",
    "            )\n",
    "            for mu, lambd, a, b in zip(self.mu, self.lambd, self.alpha, self.beta)\n",
    "        ]\n",
    "        return max(range(self.k), key=lambda x: samples[x])\n",
    "\n",
    "thompson_sampling_agents = [\n",
    "    ThompsonSamplingAgent(T=100, k=len(bandit)) for _ in range(1000)\n",
    "]\n",
    "regrets = [agent.run(bandit) for agent in thompson_sampling_agents]\n",
    "show_result(regrets, 'Thompson Sampling Agent')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
