{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在本节中，我们实现一个Llama-2模型。\n",
    "\n",
    "## 模型实现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import torch\n",
    "import transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 位置编码"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama使用RoPE位置编码。注意此处的位置编码和RoFormer中实现的不同，RoFormer将相邻的两个元素分为一组进行旋转操作，而LlamaRope是将向量分为前后两段，前后两段相同位置的元素分为一组进行旋转操作。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RoPE(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, theta: int | float = 10000):\n",
    "        super(RoPE, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.theta = theta ** -(torch.arange(0, d_model, 2) / d_model)\n",
    "\n",
    "    @functools.lru_cache(maxsize=None)\n",
    "    def _forward_l(self, L: int) -> torch.Tensor:\n",
    "        # Use lru_cache to avoid redundant computation for the same L\n",
    "\n",
    "        D = self.d_model\n",
    "        pos = torch.einsum(\n",
    "            'l,d->ld',\n",
    "            torch.arange(L), self.theta\n",
    "        )  # [L, D / 2]\n",
    "\n",
    "        # 0 paired with d // 2, 1 paired with d // 2 + 1, ...\n",
    "        cos = torch.cos(pos).repeat([1, 2])\n",
    "        sin = torch.sin(pos)\n",
    "\n",
    "        # Here we do not use efficient method, but construct a rotary matrix\n",
    "        result = torch.zeros(L, D, D)\n",
    "        result = torch.diagonal_scatter(result, cos, dim1=1, dim2=2)\n",
    "        result = torch.diagonal_scatter(result, sin, dim1=1, dim2=2, offset=-D // 2)\n",
    "        result = torch.diagonal_scatter(result, -sin, dim1=1, dim2=2, offset=D // 2)\n",
    "        return result\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # x: [N, L, H, D]\n",
    "        _, L, _, D = x.size()\n",
    "        rot_matrix = self._forward_l(L).to(x.device)  # [L, D, D]\n",
    "        return torch.einsum('lde,nlhe->nlhd', rot_matrix, x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 注意力机制"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RoPE是在多头注意力机制分头后才进行计算。并且，Llama的注意力机制中不包含偏置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int, rope: RoPE):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.sqrt_d_k = self.d_k ** 0.5\n",
    "        self.rope = rope\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "        self.W_O = torch.nn.Linear(d_model, d_model, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self, x_q: torch.Tensor, x_k: torch.Tensor, x_v: torch.Tensor,\n",
    "        padding_mask: torch.Tensor | None = None,\n",
    "        attention_mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        # x_q: (N, L_Q, D), x_k: (N, L_KV, D), x_v: (N, L_KV, D)\n",
    "        # padding_mask: (N, L_KV), attention_mask: (L_Q, L_KV)\n",
    "        N, L_Q, D = x_q.size()\n",
    "        _, L_KV, _ = x_k.size()\n",
    "        Q = self.rope(self.W_Q(x_q).reshape(N, L_Q, self.num_heads, self.d_k))\n",
    "        K = self.rope(self.W_K(x_k).reshape(N, L_KV, self.num_heads, self.d_k))\n",
    "        V = self.W_V(x_v).reshape(N, L_KV, self.num_heads, self.d_k)\n",
    "\n",
    "        score = torch.einsum('nihd,njhd->nijh', Q, K) / self.sqrt_d_k\n",
    "        # score: (N, L_Q, L_KV, num_heads)\n",
    "        # Apply attention mask\n",
    "        if attention_mask is not None:\n",
    "            score = score.masked_fill(\n",
    "                attention_mask.reshape(1, L_Q, L_KV, 1) == 0, float('-inf')\n",
    "            )\n",
    "        # Apply padding mask\n",
    "        if padding_mask is not None:\n",
    "            score = score.masked_fill(\n",
    "                padding_mask.reshape(N, 1, L_KV, 1) == 0, float('-inf')\n",
    "            )\n",
    "        score = torch.nn.functional.softmax(score, dim=2)\n",
    "        value = torch.einsum(\n",
    "            'nijh,njhd->nihd', score, V\n",
    "        ).reshape(N, L_Q, self.d_model)\n",
    "        return self.W_O(value)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, d_model: int, num_heads: int, rope=RoPE):\n",
    "        super(MultiHeadSelfAttention, self).__init__(d_model, num_heads, rope)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor,\n",
    "        padding_mask: torch.Tensor | None = None,\n",
    "        attention_mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        return super().forward(x, x, x, padding_mask, attention_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FFN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在前馈网络部分，Llama使用Swiglu作为激活函数。此部分同样不使用偏置"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, input_dim, bias=False)\n",
    "        self.gate = torch.nn.Linear(input_dim, hidden_dim, bias=False)\n",
    "        self.act = torch.nn.SiLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.act(self.gate(x)) * self.fc1(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LayerNorm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama使用RMSNorm作为LayerNorm。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RMSNorm(torch.nn.Module):\n",
    "    def __init__(self, dim: int, eps: float = 1e-8):\n",
    "        super(RMSNorm, self).__init__()\n",
    "        self.weight = torch.nn.Parameter(torch.ones(dim))\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        return x / torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True) + self.eps) * self.weight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Llama使用Pre-norm，即先进行norm，再进行attention/FFN操作，最后相加。据此可以构建出LlamaDecoder的基本结构。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaDecoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, d_model: int, num_heads: int, rope: RoPE, hidden_dim: int, layernorm_eps: float\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.norm1 = RMSNorm(d_model, layernorm_eps)\n",
    "        self.attention = MultiHeadSelfAttention(d_model, num_heads, rope)\n",
    "        self.norm2 = RMSNorm(d_model, layernorm_eps)\n",
    "        self.ffn = FFN(d_model, hidden_dim)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, padding_mask: torch.Tensor | None = None, attention_mask: torch.Tensor | None = None):\n",
    "        hidden = x\n",
    "        hidden += self.attention(self.norm1(hidden), padding_mask, attention_mask)\n",
    "        hidden += self.ffn(self.norm2(hidden))\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Llama模型构建"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LlamaModel(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, config: transformers.LlamaConfig\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.embedding = torch.nn.Embedding(config.vocab_size, config.hidden_size)\n",
    "        self.rope = RoPE(\n",
    "            config.hidden_size // config.num_attention_heads,\n",
    "            config.rope_theta\n",
    "        )\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            LlamaDecoder(\n",
    "                config.hidden_size,\n",
    "                config.num_attention_heads,\n",
    "                self.rope,\n",
    "                config.intermediate_size,\n",
    "                config.rms_norm_eps\n",
    "            )\n",
    "            for _ in range(config.num_hidden_layers)\n",
    "        ])\n",
    "        self.norm = RMSNorm(config.hidden_size)\n",
    "        self.lm_head = torch.nn.Linear(config.hidden_size, config.vocab_size, bias=False)\n",
    "\n",
    "    def forward(\n",
    "        self, input_ids: torch.Tensor,\n",
    "        padding_mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        hidden = self.embedding(input_ids)\n",
    "        attention_mask = 1 - torch.triu(\n",
    "            torch.ones(hidden.size(1), hidden.size(1)), diagonal=1\n",
    "        ).to(hidden.device)\n",
    "        for layer in self.layers:\n",
    "            hidden = layer(hidden, padding_mask, attention_mask)\n",
    "        hidden = self.norm(hidden)\n",
    "        hidden = self.lm_head(hidden)\n",
    "        return hidden"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型验证\n",
    "\n",
    "首先，从`Llama-2-7b-hf`中加载模型配置，并根据配置初始化模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/dist-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "llama_config = transformers.AutoConfig.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "llama = LlamaModel(llama_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载Huggingface提供的Llama模型，并且将预训练的Llama参数复制到实现的模型中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daf152debbe1474c852dfcde91c1100a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "hf_llama = transformers.LlamaForCausalLM.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "\n",
    "def load_params(custom_model: LlamaModel, hf_model: transformers.LlamaForCausalLM):\n",
    "    layer_pairs = [\n",
    "        (custom_model.embedding, hf_model.model.embed_tokens),\n",
    "        (custom_model.lm_head, hf_model.lm_head),\n",
    "        (custom_model.norm, hf_model.model.norm)\n",
    "    ]\n",
    "    for custom_layer, hf_layer in zip(custom_model.layers, hf_model.model.layers):\n",
    "        layer_pairs.extend([\n",
    "            (custom_layer.norm1, hf_layer.input_layernorm),\n",
    "            (custom_layer.norm2, hf_layer.post_attention_layernorm),\n",
    "            (custom_layer.attention.W_Q, hf_layer.self_attn.q_proj),\n",
    "            (custom_layer.attention.W_K, hf_layer.self_attn.k_proj),\n",
    "            (custom_layer.attention.W_V, hf_layer.self_attn.v_proj),\n",
    "            (custom_layer.attention.W_O, hf_layer.self_attn.o_proj),\n",
    "            (custom_layer.ffn.gate, hf_layer.mlp.gate_proj),\n",
    "            (custom_layer.ffn.fc1, hf_layer.mlp.up_proj),\n",
    "            (custom_layer.ffn.fc2, hf_layer.mlp.down_proj)\n",
    "        ])\n",
    "    for custom_layer, hf_layer in layer_pairs:\n",
    "        custom_layer.weight.data = hf_layer.weight.data.clone().detach()\n",
    "    return custom_model\n",
    "\n",
    "llama = load_params(llama, hf_llama)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载tokenizer并生成输入数据，注意Llama-2没有padding token，需要手动设置。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_tokenizer = transformers.LlamaTokenizer.from_pretrained('meta-llama/Llama-2-7b-hf')\n",
    "llama_tokenizer.pad_token = llama_tokenizer.eos_token\n",
    "tokenized_sentence = llama_tokenizer([\n",
    "    'User: Hello Llama! How are you?\\nAssistant:',\n",
    "    'User: Hello World!\\nAssistant:'\n",
    "], return_tensors='pt', padding=True)\n",
    "input_ids = tokenized_sentence['input_ids']\n",
    "padding_mask = tokenized_sentence['attention_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "根据输入数据，计算每层模型的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_layers_output = hf_llama(input_ids, attention_mask=padding_mask, output_hidden_states=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下对输出结果进行验证。首先验证embedding层的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_layers_output.hidden_states[0]\n",
    "hidden = llama.embedding(input_ids)\n",
    "attention_mask = 1 - torch.triu(\n",
    "    torch.ones(hidden.size(1), hidden.size(1)), diagonal=1\n",
    ").to(device=hidden.device)\n",
    "\n",
    "# Hidden state 1 equivalent to embedding output\n",
    "torch.allclose(\n",
    "    llama_layers_output.hidden_states[0],\n",
    "    hidden\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证每一层decoder的输出，最后一层decoder可能会出现不同。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, True)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify layer outputs\n",
    "layer_results = []\n",
    "for i in range(llama_config.num_hidden_layers - 1):\n",
    "    hidden = llama.layers[i](hidden, padding_mask, attention_mask)\n",
    "    layer_results.append(torch.allclose(\n",
    "        llama_layers_output.hidden_states[i + 1], hidden, atol=1e-4\n",
    "    ))\n",
    "all(layer_results), all(layer_results[:-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "验证LM-head的输出。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Verify output logits\n",
    "logits = llama(input_ids, padding_mask)\n",
    "hf_logits = llama_layers_output.logits\n",
    "torch.allclose(logits, hf_logits, atol=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "尝试对输出的单词进行解码。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I\n",
      "Hello\n",
      "Fine\n",
      "Hi\n",
      "Oh\n"
     ]
    }
   ],
   "source": [
    "# Decode the output tokens\n",
    "max_prob_tokens = logits[0, -1].topk(5).indices.tolist()\n",
    "for _ in max_prob_tokens:\n",
    "    print(llama_tokenizer.decode(_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "21"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del hf_llama\n",
    "del llama_layers_output\n",
    "del hf_logits\n",
    "import gc\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 模型推理\n",
    "\n",
    "在推理阶段，模型根据输入序列，不断预测序列中的下一个单词，并且将其加入序列中。重复这个过程，直到出现结束token。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def complete_ids(model: LlamaModel, input_ids: torch.Tensor):\n",
    "    assert input_ids.size(0) == 1\n",
    "\n",
    "    next_token = model(input_ids, torch.ones_like(input_ids))[0, -1, :].topk(1).indices[0].item()\n",
    "    return next_token\n",
    "\n",
    "def complete_string(model: LlamaModel, tokenizer: transformers.LlamaTokenizer, prompt: str, max_length: int):\n",
    "    model.eval()\n",
    "    gc.collect()\n",
    "    tokenized_sentence = tokenizer([prompt], return_tensors='pt')['input_ids'].tolist()\n",
    "    next_token = None\n",
    "    print(tokenizer.decode(tokenized_sentence[0]))\n",
    "    num_tokens = 0\n",
    "\n",
    "    while next_token != tokenizer.eos_token_id and num_tokens <= max_length:\n",
    "        num_tokens += 1\n",
    "        next_token = complete_ids(model, torch.tensor(tokenized_sentence))\n",
    "        tokenized_sentence[0].append(next_token)\n",
    "        print(tokenizer.decode(tokenized_sentence[0]))\n",
    "\n",
    "        gc.collect()\n",
    "\n",
    "    return num_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant:\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is \n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 1\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 13\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 13.\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 13.8\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 13.8.\n",
      "<s> User: Given two numbers, 13.11 and 13.8, which is larger?\n",
      "Assistant: The larger number is 13.8.\n",
      "\n",
      "Predicted 11 in 37.42 seconds, 0.29 tokens/s\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "query = 'User: Given two numbers, 13.11 and 13.8, which is larger?\\nAssistant:'\n",
    "\n",
    "start_time = time.time()\n",
    "num_tokens = complete_string(llama, llama_tokenizer, query, 10)\n",
    "end_time = time.time()\n",
    "\n",
    "time_span = end_time - start_time\n",
    "print(f'Predicted {num_tokens} in {time_span:.2f} seconds, {num_tokens / time_span:.2f} tokens/s')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
