# 强化学习基础概念

强化学习是用于解决序列决策任务的一种机器学习方式。

## 序列决策任务

序列决策（Sequential Decision Making）是强化学习的核心概念。它涉及到在一个环境中，智能体（Agent）通过与环境的交互来学习如何做出决策。智能体自身有一个状态（State）$s$，并根据当前状态选择一个动作（Action）$a$。环境根据智能体的动作返回一个奖励（Reward）$r$和下一个状态$s'$。策略（Policy）$\pi(\theta)$是智能体在给定状态下选择动作的规则，可以是确定性的（Deterministic）或随机的（Stochastic）。

智能体的目标是最大化其在$T$周期内的累积奖励（Cumulative Reward），通常表示为折扣奖励（Discounted Reward）$r_t$。状态价值（State Value）$V_\pi(s)$表示在策略$\pi$下，从状态$s$出发，智能体在未来的所有时间步内所能获得的期望奖励。

$$
\begin{aligned}
& \max_\pi V_\pi (s_0):= \bbE_{a_0, \ldots, s_T, a_T\mid s_0}\left[\sum_{t=0}^{T} \gamma^t r_t(s_t, a_t)\middle| s_0\right] \\
s.t. & \left\{
\begin{aligned}
& s_{t+1} \sim P(s_{t+1} | s_t, a_t) \\
& a_t \sim \pi(s_t) \\
\end{aligned}
\right.
\end{aligned}
$$

折扣因子（Discount Factor）$\gamma$是一个介于0和1之间的值，用于平衡智能体关注短期奖励和长期奖励的程度。当$\gamma$接近1时，智能体更关注长期奖励；当$\gamma$接近0时，智能体更关注短期奖励。

另一种定义智能体优化目标的方式为最小化后悔值（Regret），即在$T$个时间步内，智能体所获得的奖励与最优策略所能获得的奖励之间的差值，即

$$
\min_\pi V_{\pi^*} (S_0) - V_\pi (S_0)
$$

## 问题建模

强化学习可以被建模成马尔可夫决策过程（Markov Decision Process, MDP）的变种。考虑到交互过程中环境自身的变化。智能体和环境的交互过程可以更一般地被建模为如下流程：

1. 智能体的内部状态是对环境的信念，每个时间步内，智能体根据策略选择一个动作$a_t = \pi(s_t; \theta_t)$。
2. 环境根据动作$a_t$，更新自身状态$e_{t+1} = W(e_t, a_t)$，并返回一个智能体观测到的状态$o_{t + 1} = O(e_{t + 1})$，以及一个奖励$r_t = R(e_t, a_t, e_{t + 1})$。
3. 智能体根据当前状态$s_t$，动作$a_t$，预测对下一个状态的信念估计$\hat s_{t + 1} = P(s_t, a_t)$，
4. 智能体根据观测状态$o_{t + 1}$预测环境的内部状态$\hat e_{t + 1} = D(o_{t + 1})$
5. 智能体根据信念估计和环境估计，更新自身的信念$s_{t + 1} = U(\hat s_{t + 1}, \hat e_{t + 1})$。
5. 智能体根据算法更新策略参数$\theta_{t+1} = \pi^{\text{RL}}(\theta_t, s_t, a_t, r_{t}, o_{t + 1})$。

智能体侧包含策略更新模型$\pi^{\text{RL}}$、策略模型$\pi$、预测模块$P$、状态解码$D$和信念更新模型$U$，环境模型包含状态转移模型$W$、观测模型$O$和奖励模型$R$。整体来看，更新过程包含三个随机过程，即内部状态更新$s_t$，环境状态更新$e_t$和策略更新$\theta_t$。

### POMDP

部分可观测马尔可夫决策过程（Partially Observable Markov Decision Process, POMDP）中，环境的完整状态$e_t$是不可观测的，智能体只能通过部分观测环境，更新自身对环境的信念。在以上表示下，POMDP可以被表示为五元组$(S, A, O, W, R)$，其中$S$是状态空间，$A$是动作空间，$O$是观测空间，$W$是环境状态转移函数，$R$是奖励函数。

POMDP的一个特例是上下文DP。在POMDP中，若存在一个不变的因素$C$影响和控制MDP的状态转移和奖励函数时，则为上下文MDP（Contextual MDP）。相比于POMDP，Contextual MDP更强调的是“跨环境的泛化”。


### MDP

马尔可夫决策过程（Markov Decision Process, MDP）是强化学习的基本模型，在MDP中，智能体可以完整观测到环境状态，即$s_t\equiv e_t\equiv o_t$。此时问题可以简化为四元组$(S, A, W, R)$，其中$S$是状态空间，$A$是动作空间，$R$是奖励函数，$W$是状态转移函数。

### Contextual MDP

上下文MDP（Contextual MDP）是强化学习的一个特例。智能体需要应对一系列的MDP问题，每个问题都有一个常量$C$影响和控制该MDP的状态转移和奖励函数。因此，智能体需要借助上下文信息，将策略泛化到一类MDP问题上。

### Contextual Bandit

上下文Bandit（Contextual Bandit）是POMDP的一个特例，相当于环境转移后状态$s_{t + 1}$独立于当前状态$s_t$和动作$a_t$，并且智能体能够完全观测到环境状态（$s_t\equiv o_t$），则是一个上下文bandit问题。该状态$s_t$称为“上下文”。智能体的决策仅能影响其在每一步中获得的奖励。进一步地，如果上下文不存在变化，则退化为普通多臂老虎机问题。

### Belief State MDP

在belief state MDP中，智能体维护一个环境状态$w$的估计分布，也称为信念（Belief State）$B_t$，而不是直接维护环境状态$e_t$。智能体根据该信念分布$w\sim B_t$估计奖励函数$R(o, a; w)$。在观测到新的状态后，智能体按照贝叶斯更新的方式更新信念状态。

## 强化学习

通常地，强化学习可以按照如下规则分类：

1. 智能体学习到的是状态-价值映射（value-based RL），还是状态-动作映射（policy-based RL），或者是直接学习环境（model-based RL）。
2. 智能体是通过列表的方式学习到映射，还是通过函数估计的方式学习映射。
3. 智能体选择的动作由智能体的当前策略决定（on policy），还是由任何一个策略决定（off policy）。

在智能体进行学习的过程中，需要在环境模型未知的情况下，决定是探索环境模型的更多可能性，还是保持当前的最优策略。该问题称为探索-利用权衡（exploration-exploitation policy）。若智能体过于倾向探索，则会导致多次选择偏离最优策略的动作，而若智能体过于倾向利用，则会错过更好的动作。

奖励是驱动智能体选择策略的函数，一般来说，我们可以在智能体达到我们所期待的目标时给予奖励。然而，在部分场景中，可能会出现智能体的奖励虽然很高，但实际行为和预期不一致的情况，称为reward hacking。另外，智能体也并不一定在每一步决策中都能获得奖励。当智能体获得奖励的频率很少（即奖励稀疏，sparse reward）时，模型的训练可能会难以收敛。因此，奖励不仅反映了我们期望的决策目标，还应当足够完备地反映我们对智能体决策路径的偏好。如，约束状态价值函数需要满足特定形式的单调性，或是在智能体的每一步决策中给予一个较小的奖励。