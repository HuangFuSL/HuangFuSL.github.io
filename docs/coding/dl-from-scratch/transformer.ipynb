{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先实现多头注意力机制。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(torch.nn.Module):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        if d_model % num_heads != 0:\n",
    "            raise ValueError(\"d_model must be divisible by num_heads\")\n",
    "\n",
    "        self.d_k = d_model // num_heads\n",
    "        self.sqrt_d_k = self.d_k ** 0.5\n",
    "\n",
    "        self.W_Q = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_K = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_V = torch.nn.Linear(d_model, d_model)\n",
    "        self.W_O = torch.nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(\n",
    "        self, x_q: torch.Tensor, x_k: torch.Tensor, x_v: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        N, L_Q, D = x_q.size()\n",
    "        _, L_KV, _ = x_k.size()\n",
    "        Q = self.W_Q(x_q).reshape(N, L_Q, self.num_heads, self.d_k)\n",
    "        K = self.W_K(x_k).reshape(N, L_KV, self.num_heads, self.d_k)\n",
    "        V = self.W_V(x_v).reshape(N, L_KV, self.num_heads, self.d_k)\n",
    "\n",
    "        score = torch.einsum(\"nihd,njhd->nijh\", Q, K) / self.sqrt_d_k\n",
    "        if mask is not None:\n",
    "            # mask: (L, L)\n",
    "            mask_value = mask.masked_fill(mask == 0, float(\"-inf\"))\n",
    "            score = score * mask_value.reshape(1, L_Q, L_KV, 1)\n",
    "        score = torch.nn.functional.softmax(score, dim=2)\n",
    "        value = torch.einsum(\"nijh,njhd->nihd\", score,\n",
    "                             V).reshape(N, L_Q, self.d_model)\n",
    "        return self.W_O(value)\n",
    "\n",
    "\n",
    "class MultiHeadSelfAttention(MultiHeadAttention):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super(MultiHeadSelfAttention, self).__init__(d_model, num_heads)\n",
    "\n",
    "    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None) -> torch.Tensor:\n",
    "        return super().forward(x, x, x, mask)\n",
    "\n",
    "\n",
    "class MultiHeadCrossAttention(MultiHeadAttention):\n",
    "    def __init__(self, d_model: int, num_heads: int):\n",
    "        super(MultiHeadCrossAttention, self).__init__(d_model, num_heads)\n",
    "\n",
    "    def forward(\n",
    "        self, x_q: torch.Tensor, x_kv: torch.Tensor,\n",
    "        mask: torch.Tensor | None = None\n",
    "    ) -> torch.Tensor:\n",
    "        return super().forward(x_q, x_kv, x_kv, mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "前馈神经网络由两个全连接层和ReLU激活函数组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FFN(torch.nn.Module):\n",
    "    def __init__(self, input_dim: int, hidden_dim: int):\n",
    "        super(FFN, self).__init__()\n",
    "        self.fc1 = torch.nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = torch.nn.Linear(hidden_dim, input_dim)\n",
    "        self.relu = torch.nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "编码器层由自注意力c层和前馈神经网络层组成，一个编码器由多个这样的层串联组成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, num_heads: int,\n",
    "        ffn_dim: int | None = None, dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-6\n",
    "    ):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        if ffn_dim is None:\n",
    "            ffn_dim = input_dim * 4\n",
    "\n",
    "        self.attention = MultiHeadSelfAttention(input_dim, num_heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(input_dim, layer_norm_eps)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn = FFN(input_dim, ffn_dim)\n",
    "        self.norm2 = torch.nn.LayerNorm(input_dim, layer_norm_eps)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = x + self.dropout1(self.attention(x, mask))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout2(self.ffn(x))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Encoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, input_dim: int, num_heads: int,\n",
    "        ffn_dim: int | None = None, dropout: float = 0.1,\n",
    "        layer_norm_eps: float = 1e-6\n",
    "    ):\n",
    "        super(Encoder, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            EncoderLayer(input_dim, num_heads, ffn_dim,\n",
    "                         dropout, layer_norm_eps)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        # x: (N, L, D)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "解码器层由自注意力层，交叉注意力层和前馈神经网络层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, input_dim: int, num_heads: int,\n",
    "        ffn_dim: int | None = None, dropout: float = 0.1\n",
    "    ):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        if ffn_dim is None:\n",
    "            ffn_dim = input_dim * 4\n",
    "\n",
    "        self.self_attention = MultiHeadSelfAttention(input_dim, num_heads)\n",
    "        self.norm1 = torch.nn.LayerNorm(input_dim)\n",
    "        self.dropout1 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.cross_attention = MultiHeadCrossAttention(input_dim, num_heads)\n",
    "        self.norm2 = torch.nn.LayerNorm(input_dim)\n",
    "        self.dropout2 = torch.nn.Dropout(dropout)\n",
    "\n",
    "        self.ffn = FFN(input_dim, ffn_dim)\n",
    "        self.norm3 = torch.nn.LayerNorm(input_dim)\n",
    "        self.dropout3 = torch.nn.Dropout(dropout)\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, memory: torch.Tensor, tgt_mask: torch.Tensor | None = None\n",
    "    ):\n",
    "        if tgt_mask is None:\n",
    "            tgt_mask = torch.triu(\n",
    "                torch.ones(x.size(1), x.size(1)), diagonal=1\n",
    "            ).to(x.device)\n",
    "\n",
    "        x = x + self.dropout1(self.self_attention(x, tgt_mask))\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        x = x + self.dropout2(self.cross_attention(x, memory))\n",
    "        x = self.norm2(x)\n",
    "\n",
    "        x = x + self.dropout3(self.ffn(x))\n",
    "        x = self.norm3(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Decoder(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, input_dim: int, num_heads: int,\n",
    "        ffn_dim: int | None = None, dropout: float = 0.1\n",
    "    ):\n",
    "        super(Decoder, self).__init__()\n",
    "\n",
    "        self.layers = torch.nn.ModuleList([\n",
    "            DecoderLayer(input_dim, num_heads, ffn_dim, dropout)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "\n",
    "    def forward(self, x, memory, tgt_mask):\n",
    "        # x: (N, L, D)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, tgt_mask)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformer模型由编码器和解码器组成，编码器处理源序列，将编码后的序列输入到解码器中，解码器生成目标序列。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomTransformer(torch.nn.Module):\n",
    "    def __init__(\n",
    "        self, num_layers: int, num_decoder_layers: int, input_dim: int, num_heads: int,\n",
    "        ffn_dim: int | None = None, dropout: float = 0.1, layer_norm_eps: float = 1e-6\n",
    "    ):\n",
    "        super(CustomTransformer, self).__init__()\n",
    "        self.encoder = Encoder(num_layers, input_dim, num_heads, ffn_dim, dropout, layer_norm_eps)\n",
    "        self.decoder = Decoder(num_decoder_layers, input_dim, num_heads, ffn_dim, dropout)\n",
    "\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        memory = self.encoder(src, src_mask)\n",
    "        return self.decoder(tgt, memory, tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将PyTorch内部实现的Transformer权重复制到实现的Transformer中。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _attn_load_from_torch(\n",
    "    custom_attn: MultiHeadAttention, torch_attn: torch.nn.MultiheadAttention\n",
    "):\n",
    "    embed_dim = custom_attn.d_model\n",
    "\n",
    "    def split_qkv(weight, embed_dim):\n",
    "        return weight[:embed_dim], weight[embed_dim: 2 * embed_dim], weight[2 * embed_dim:3 * embed_dim]\n",
    "    custom_attn.W_Q.weight.data, \\\n",
    "        custom_attn.W_K.weight.data, \\\n",
    "        custom_attn.W_V.weight.data = split_qkv(\n",
    "            torch_attn.in_proj_weight.data, embed_dim)\n",
    "\n",
    "    custom_attn.W_Q.bias.data, \\\n",
    "        custom_attn.W_K.bias.data, \\\n",
    "        custom_attn.W_V.bias.data = split_qkv(\n",
    "            torch_attn.in_proj_bias.data, embed_dim)\n",
    "\n",
    "    custom_attn.W_O.weight.data = torch_attn.out_proj.weight.data\n",
    "    custom_attn.W_O.bias.data = torch_attn.out_proj.bias.data\n",
    "\n",
    "    return custom_attn\n",
    "\n",
    "\n",
    "def load_from_torch(\n",
    "    custom_transformer: CustomTransformer, torch_transformer: torch.nn.Transformer\n",
    "):\n",
    "    for custom_layer, torch_layer in zip(\n",
    "        [*custom_transformer.encoder.layers, *custom_transformer.decoder.layers],\n",
    "        [*torch_transformer.encoder.layers, *torch_transformer.decoder.layers],\n",
    "    ):\n",
    "\n",
    "        if hasattr(custom_layer, 'cross_attention'):\n",
    "            # Decoder\n",
    "            custom_layer.self_attention = _attn_load_from_torch(\n",
    "                custom_layer.self_attention, torch_layer.self_attn\n",
    "            )\n",
    "            custom_layer.cross_attention = _attn_load_from_torch(\n",
    "                custom_layer.cross_attention, torch_layer.multihead_attn\n",
    "            )\n",
    "        else:\n",
    "            # Encoder\n",
    "            custom_layer.attention = _attn_load_from_torch(\n",
    "                custom_layer.attention, torch_layer.self_attn\n",
    "            )\n",
    "\n",
    "        layer_pairs = [\n",
    "            (custom_layer.norm1, torch_layer.norm1),\n",
    "            (custom_layer.ffn.fc1, torch_layer.linear1),\n",
    "            (custom_layer.ffn.fc2, torch_layer.linear2),\n",
    "            (custom_layer.norm2, torch_layer.norm2)\n",
    "        ]\n",
    "        for custom, torch in layer_pairs:\n",
    "            custom.weight.data = torch.weight.data\n",
    "            custom.bias.data = torch.bias.data\n",
    "    return custom_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通过代码验证实现的Transformer模型的正确性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([-0.0734,  0.6697, -0.5964, -1.5128,  0.1729], grad_fn=<SliceBackward0>),\n",
       " tensor([-0.0734,  0.6697, -0.5964, -1.5128,  0.1729], grad_fn=<SliceBackward0>))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transformer_config = {\n",
    "    'num_layers': 6,\n",
    "    'num_decoder_layers': 6,\n",
    "    'input_dim': 512,\n",
    "    'num_heads': 8,\n",
    "    'ffn_dim': 2048,\n",
    "    'dropout': 0.1,\n",
    "    'layer_norm_eps': 1e-6\n",
    "}\n",
    "\n",
    "custom_transformer = CustomTransformer(**transformer_config)\n",
    "torch_transformer = torch.nn.Transformer(\n",
    "    d_model=transformer_config['input_dim'],\n",
    "    nhead=transformer_config['num_heads'],\n",
    "    num_encoder_layers=transformer_config['num_layers'],\n",
    "    num_decoder_layers=transformer_config['num_decoder_layers'],\n",
    "    dim_feedforward=transformer_config['ffn_dim'],\n",
    "    dropout=transformer_config['dropout'],\n",
    "    batch_first=True\n",
    ")\n",
    "custom_transformer = load_from_torch(custom_transformer, torch_transformer)\n",
    "custom_transformer.eval()\n",
    "torch_transformer.eval()\n",
    "\n",
    "src = torch.randn(32, 10, transformer_config['input_dim'])\n",
    "tgt = torch.randn(32, 20, transformer_config['input_dim'])\n",
    "src_mask = torch.ones(10, 10)\n",
    "tgt_mask = torch.ones(20, 20)\n",
    "\n",
    "custom_output = custom_transformer(src, tgt, src_mask, tgt_mask)\n",
    "torch_output = torch_transformer(src, tgt, src_mask, tgt_mask)\n",
    "custom_output[0, 0, :5], torch_output[0, 0, :5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.allclose(custom_output, torch_output, atol=1e-4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
