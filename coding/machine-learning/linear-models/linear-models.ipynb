{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 线性模型\n",
    "\n",
    "以下使用Python实现基于线性关系的各种机器学习模型\n",
    "\n",
    "## 多元线性回归\n",
    "\n",
    "多元线性回归的一般形式如下：\n",
    "\n",
    "$$\n",
    "\\hat{\\boldsymbol y} = f(\\boldsymbol x) = \\boldsymbol w^\\top \\boldsymbol x + b\n",
    "$$\n",
    "\n",
    "当样本矩阵$\\boldsymbol X$满秩时，最优解为\n",
    "\n",
    "$$\n",
    "\\newcommand{\\bmX}{\\boldsymbol X}\n",
    "\\hat{\\boldsymbol w}^* = (\\bmX^\\top\\bmX)^{-1}\\bmX^\\top \\boldsymbol y\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/melon3.0.csv')\n",
    "X_rcol, y_rcol = df.columns[1:-2], df.columns[-2] # columns used for regression\n",
    "X_ccol, y_ccol = df.columns[1:-1], df.columns[-1] # columns used for classification\n",
    "value_map = {\n",
    "    '色泽': {'浅白': 0, '青绿': 1, '乌黑': 2},\n",
    "    '根蒂': {'蜷缩': 0, '稍蜷': 1, '硬挺': 2},\n",
    "    '敲声': {'沉闷': 0, '浊响': 1, '清脆': 2},\n",
    "    '纹理': {'模糊': 0, '稍糊': 1, '清晰': 2},\n",
    "    '脐部': {'凹陷': 0, '稍凹': 1, '平坦': 2},\n",
    "    '触感': {'硬滑': 0, '软粘': 1},\n",
    "    '好瓜': {'是': 1, '否': 0},\n",
    "}\n",
    "for col in value_map:\n",
    "    df[col] = df[col].map(value_map[col])\n",
    "\n",
    "X = np.concatenate([df[X_rcol].values, np.ones((df.shape[0], 1))], axis=1)\n",
    "y = df[y_rcol].values.reshape(-1, 1)\n",
    "\n",
    "def linear_regression(X, y):\n",
    "    return np.linalg.inv(X.T @ X) @ X.T @ y\n",
    "\n",
    "linear_regression(X, y).T"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "损失函数为均方误差"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse_loss(X, y, w):\n",
    "    return np.mean((X @ w - y) ** 2)\n",
    "\n",
    "mse_loss(X, y, linear_regression(X, y))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当样本矩阵非满秩时，存在多个满足训练集的模型，此时可以在优化目标中加入正则化项，如L2-norm即加入权重的平方之和。然后使用梯度下降等数值方式进行计算。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    def __init__(self, d, lr=0.01, epochs=1000):\n",
    "        self.d = d\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "\n",
    "    def __call__(self, X, y):\n",
    "        w = np.random.normal(0, 1, size=(X.shape[1], 1))\n",
    "        for _ in range(self.epochs):\n",
    "            w -= self.lr * self.d(X, y, w)\n",
    "        return w\n",
    "\n",
    "optim_l2 = SGD(lambda X, y, w: X.T @ (X @ w - y) + 0.1 * w)\n",
    "\n",
    "w = optim_l2(X, y)\n",
    "print(w.T, mse_loss(X, y, w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "对率回归可以将线性模型应用到二分类问题上。对率回归需要用到logit函数将连续的回归值映射到 $(0, 1)$ 上\n",
    "\n",
    "$$\n",
    "f(x) = \\frac{1}{1 + e^{-x}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[X_ccol].values\n",
    "y = df[y_ccol].values.reshape(-1, 1)\n",
    "\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def precision(X, y, w):\n",
    "    return np.mean((sigmoid(X @ w) > 0.5) == y)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用极大似然法可以得到对率回归的优化目标函数\n",
    "\n",
    "$$\n",
    "l(\\boldsymbol w) = \\sum_{i=1}^m \\left(-\\boldsymbol y_i\\boldsymbol \\beta^\\top \\boldsymbol x_i + \\ln \\left(1 + e^{\\boldsymbol \\beta^\\top \\boldsymbol x_i}\\right) \\right)\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optim_logit = SGD(lambda X, y, w: X.T @ (sigmoid(X @ w) - y))\n",
    "\n",
    "w = optim_logit(X, y)\n",
    "print(w.T, precision(X, y, w))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LDA\n",
    "\n",
    "LDA是线性判别分析的简称，属于分类算法。该算法的核心思路为将样本点投影到 $n$ 维空间的平面上，通过选择平面，最小化同一类别内样本点投影的距离，同时最大化不同类别样本点投影的距离。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cov(X, a, b):\n",
    "    return np.mean((X[:, a] - np.mean(X[:, a])) * (X[:, b] - np.mean(X[:, b])))\n",
    "\n",
    "def cov_matrix(X):\n",
    "    return np.array([\n",
    "        [\n",
    "            cov(X, i, j)\n",
    "            for i in range(X.shape[1])\n",
    "        ]\n",
    "        for j in range(X.shape[1])\n",
    "    ])\n",
    "\n",
    "sw = cov_matrix(X[y[:, 0] == 0]) + cov_matrix(X[y[:, 0] == 1])\n",
    "mu0, mu1 = np.mean(X[y[:, 0] == 0], axis=0), np.mean(X[y[:, 0] == 1], axis=0)\n",
    "w = np.linalg.inv(sw) @ (mu0 - mu1).reshape(-1, 1)\n",
    "c0, c1 = w.T @ mu0, w.T @ mu1\n",
    "\n",
    "def precision(X, y, w):\n",
    "    return np.mean((X @ w < (c0 + c1) / 2) == y)\n",
    "\n",
    "precision(X, y, w)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
